Whether it's triaging alerts, monitoring systems, or analyzing log data during incident investigations, a SIEM is the tool for this job. A **SIEM** is an application that collects and analyzes log data to monitor critical activities in an organization. It does this by collecting, analyzing, and reporting on security data from multiple sources.

### Process of collection of data in a SIEM
- SIEM tools *COLLECT AND PROCESS* enormous amounts of data generated by devices and systems from all over an environment. 
- Not all data is the same. Devices generate data in different formats. This can be challenging because there is no unified format to represent the data. SIEM tools make it easy for security analysts to read and analyze data by *NORMALIZING* it. Raw data gets processed, so that it's formatted consistently and only relevant event information is included.
- SIEM tools *INDEX* the data, so it can be accessed through search. All of the events across all the different sources can be accessed with just a search. 

SIEM tools make it easy to quickly access and analyze the data flows happening across networks in an environment. As a security analyst, we may encounter different SIEM tools.
It's important that we're able to adjust and adapt to whichever tool our organization ends up using. 
## Log sources and log ingestion

1. **Collect and aggregate data**: SIEM tools collect event data from various data sources.
    
2. **Normalize data**: Event data that's been collected becomes normalized. Normalization converts data into a standard format so that data is structured in a consistent way and becomes easier to read and search. While data normalization is a common feature in many SIEM tools, it's important to note that SIEM tools vary in their data normalization capabilities.
    
3. **Analyze data**: After the data is collected and normalized, SIEM tools analyze and correlate the data to identify common patterns that indicate unusual activity.
    

This focuses on the first step of this process, the collection and aggregation of data.

## Log ingestion
![[loginjestion.png]]

Data is required for SIEM tools to work effectively. SIEM tools must first collect data using log ingestion. Log ingestion is the *process of collecting and importing data from log sources into a SIEM tool*. Data comes from any source that generates log data, like a server.

In log ingestion, *the SIEM creates a copy of the event data it receives and retains it within its own storage*. This copy allows the SIEM to analyze and process the data without directly modifying the original source logs. The collection of event data provides a centralized platform for security analysts to analyze the data and respond to incidents. This event data includes authentication attempts, network activity, and more.

### Log forwarders

There are many ways SIEM tools can ingest log data. For instance, we can manually upload data or use software to help collect data for log ingestion. Manually uploading data may be inefficient and time-consuming because networks can contain thousands of systems and devices. Hence, it's easier to use software that helps collect data. 

A common way that organizations collect log data is to use log forwarders. Log forwarders are *software that automate the process of collecting and sending log data*. Some operating systems have native log forwarders. If we are using an operating system that does not have a native log forwarder, we would need to install a third-party log forwarding software on a device. After installing it, we'd configure the software to specify which logs to forward and where to send them. For example, we can configure the logs to be sent to a SIEM tool. The SIEM tool would then process and normalize the data. This allows the data to be easily searched, explored, correlated, and analyzed.

**Note**: Many SIEM tools utilize their own proprietary log forwarders. SIEM tools can also integrate with open-source log forwarders. Choosing the right log forwarder depends on many factors such as the specific requirements of wer system or organization, compatibility with wer existing infrastructure, and more.


#### Query for events in different SIEMs
Data that's been imported into a SIEM can be accessed by entering queries into the SIEM's search engine. Massive amounts of data can be stored in a SIEM database. Some of this data may date back years. This can make searching for security events challenging. For example, let's say we're searching to find a failed login event. We search for the event using the keywords: failed login. This is a very broad query, which can return thousands of results.
Broad search queries like this, slow down the response times of a search engine since it's searching across all the indexed data. But, if you specify additional parameters, like an event ID and a date and time range, you can narrow down the search to get faster results. It's important that search queries are specific, so that you can find exactly what you're  looking for and save time in the search process. Different SIEM tools use different search methods.
###  1. Splunk
Splunk is a data analysis platform. Splunk Enterprise Security provides SIEM solutions that let we search, analyze, and visualize security data. First, it collects data from different sources. That data gets processed and stored in an index. Then, it can be accessed in a variety of different ways, like through search.
##### Query for events with Splunk
Splunk uses its own query language called Search Processing Language, or SPL for short.
SPL has many different search options you can use to optimize search results, so that you can get the data you're looking for. Example: Let's take a raw log search in Splunk Cloud for events that reference errors or failures for a fictional online store called Buttercup Games.
![[splunkquery.png]]
First, we'll use the search bar to type in our query: **buttercupgames error OR fail***
This search is specifying the index, which is buttercupgames. We also specify the search terms: error OR fail. The Boolean operator OR ensures that both of the keywords will be searched.
The asterisk at the end of the term fail* is known as a *wildcard*. This means it will search for
all possible endings that contain the term fail. This helps us expand our search results because events may label failures differently. For example, some events may use the term failed.
Next, we'll select a time range using the time range picker. The more specific our search is, the better. Let's search for data from the last 30 days. Under the search bar, we have our search results. There's a timeline, which gives us a visual representation of the number of events over a period. This can be helpful in identifying event patterns such as peaks in activity. Under the timeline, there's the events viewer, which gives us a list of events that match our search. 
![[splunkquery2.png]]
Notice how our search terms: buttercupgames and error are highlighted in each event. It doesn't appear that any events matching with the term fail were found. Each event has a timestamp and raw logged data. For the events with errors, it appears that there's an error relating to the HTTP cookies used in the Buttercup Games website. 
At the bottom of the raw log data, there's some information related to the data source, including the host name, source, and source type. This information tells us where the event data originated from such as a device or file. If we click on it, we can choose to exclude it from the search results.
![[splunkremovewww1.png]]
On the search bar, we can examine that the search terms have been changed and   host!=www1 has been added, which means not to include www1 hosts. The new search results does not contain www1 as a host, but contain www2 and www3. This search is known as a raw log search. 
![[splunkchngedwww.png]]

###### More search terms
Splunk has its own querying language called **Search Processing Language (SPL)**. SPL is used to search and retrieve events from indexes using Splunk’s Search & Reporting app. An SPL search can contain many different commands and arguments. For example, you can use commands to transform your search results into a chart format or filter results for specific information. 

![Splunk Cloud's search page.](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/zmjvpMvASVuLzBl6p1-KJg_48cf882e1ea14f6ca2f3ceee91e1f2e1_cIW5xo7oKZMktF78z_u5eTeEJANj9wPgAG39QVCd8PDuvdrztqt2N1fJMbJOFms1QoIgAk0YNgHWjR1LQMLg__bhWqMMWmWke6kQmoWLpyxM5eVwNDyW7_2KttYjVSz2fYPCX4arj1TUHKrSfANwCU8?expiry=1695600000000&hmac=_6scYjFXu_L69ZL0MzlYaSmrPr5NX32oMAP4IaWscws)

Here is an example of a basic SPL search that is querying an index for a failed event:

index=main fail 

- index=main: This is the beginning of the search command that tells Splunk to retrieve events from an  index named main. An index stores event data that's been collected and processed by Splunk.
    
- fail: This is the search term. This tells Splunk to return any event that contains the term fail.
    

Knowing how to effectively use SPL has many benefits. It helps shorten the time it takes to return search results. It also helps you obtain the exact results you need from various data sources. SPL supports many different types of searches that are beyond the scope of this reading. If you would like to learn more about SPL, explore [Splunk's Search Reference](https://docs.splunk.com/Documentation/Splunk/9.0.2/SearchReference/UnderstandingSPLsyntax)

.

### **Pipes**

Previously, you might have learned about how piping is used in the Linux bash shell. As a refresher, piping sends the output of one command as the input to another command.

SPL also uses the pipe character | to separate the individual commands in the search. It's also used to chain commands together so that the output of one command combines into the next command. This is useful because you can refine data in various ways to get the results you need using a single command.

Here is an example of two commands that are piped together: 

index=main fail| chart count by host

- index=main fail: This is the beginning of the search command that tells Splunk to retrieve events from an  index named main for events containing the search term fail. 
    
- |: The pipe character separates and chains the two commands index=main and chart count by host. This means that the output of the first command index=main is used as the input of the second command chart count by host. 
    
- chart count by host: This command tells Splunk to transform the search results by creating a chart according to the  count or number of events. The argument by host tells Splunk to list the events by host, which are the names of the devices the events come from. This command can be helpful in identifying hosts with excessive failure counts in an environment.
    

### **Wildcard**

A **wildcard** is a special character that can be substituted with any other character. A wildcard is usually symbolized by an asterisk character *. Wildcards match characters in string values. In Splunk, the wildcard that you use depends on the command that you are using the wildcard with. Wildcards are useful because they can help find events that contain data that is similar but not entirely identical. Here is an example of using a wildcard to expand the search results for a search term:

index=main fail*

- index=main: This command retrieves events from an index named main. 
    
- fail*: The wildcard after fail represents any character. This tells Splunk to search for all possible endings that contain the term fail. This expands the search results to return any event that contains the term fail such as “failed” or “failure”.
    

**Pro tip**: Double quotations are used to specify a search for an exact phrase or string. For example, if you want to only search for events that contain the exact phrase login failure, you can enclose the phrase in double quotations "login failure". This search will match only events that contain the exact phrase login failure and not other events that contain the words failure or login separately.
### 2. Chronicle
Chronicle is Google Cloud's SIEM, which stores security data for search, analysis, and visualization. First, data gets forwarded to Chronicle. This data then gets normalized, or cleaned up, so it's easier to process and index. Finally, the data becomes available to be accessed through a search bar. 

##### Query for events with Chronicle
![[chroniclequery.mp4]]
Chronicle uses the YARA-L language to define rules for detection. It's a computer language used to create rules for searching through ingested log data. For example, you can use YARA-L to write a rule to detect specific activities related to the exfiltration of valuable data.
Using Chronicle's search field, you can search for fields like hostname, domain, IP, URL, email, username, or file hash. Using the search field, you can enter different types of searches. The default method of search is using UDM search, which stands for **Unified Data Model.** It searches through normalized data. If you can't find the data you're looking for searching the normalized data, you have the option of searching raw logs. Raw log search searches through the logs which have not been normalized.
Raw logs get processed during the normalization step. During normalization, all of the relevant information from raw logs gets extracted and formatted, making the data easier to search. A reason we might need to search raw logs is to find data that may not have been
included in the normalized logs, like specific fields which have not been normalized, or to troubleshoot data ingestion problems. 
To examine a UDM search for a failed login using Chronicle. First, let's click on the structured query builder icon, so that we can perform a UDM search. We'll type in the search:
`metadata.event_type = "USER_LOGIN" AND security_result.action = "BLOCK"`
Let's break down this UDM search. Since we are searching for normalized data, we need to specify a search that uses UDM format. UDM events have a set of common fields. 
The *metadata.event_type* field details the event's type. Here, we're asking Chronicle to find
an authentication activity event, a user login. 
Next, there's *AND*, which is a logical operator that tells the search engine to contain both terms.
Finally, the *security_result.action* field specifies a security action such as allow or block. Here, the action is BLOCK. This means the user login was blocked or failed.
Next, we'll press the query button. We're going to focus on searching normalized data.
We're presented with a screen with the search results. There's lots of information here.
Under UDM Search, we can observe our search terms.
There's also a bar graph timeline visualizing the failed login events over a period.
At a quick glance, this gives us a snapshot of the failed login activity over time allowing us to spot possible patterns. Under the timeline, there's a list of events with timestamps associated with this search. Under each event, there's an asset, which is the name of a device.
For example, this event shows a failed login for a user named alice. If we click the event, we can open up the raw log associated with the event. We can interpret these raw logs for more detail about the event's activity during the investigation. To the left, there's Quick Filters. These are additional fields or values that we can use to filter the search results. For example, if we click: target.ip, we are given a list of IP addresses. If we click one of these IP addresses, we can filter the search results to contain only this target IP address. This helps us find specific data we're looking for and helps us save time in the process. 

###### More search terms in chronicle
In Chronicle, you can search for events using the Search field. You can also use Procedural Filtering to apply filters to a search to further refine the search results. For example, you can use Procedural Filtering to include or exclude search results that contain specific information relating to an event type or log source. There are two types of searches you can perform to find events in Chronicle, a Unified Data Mode (UDM) Search or a Raw Log Search.
![[chronicleui.png]]

### **Unified Data Model (UDM) Search**

The UDM Search is the default search type used in Chronicle. You can perform a UDM search by typing your search, clicking on “Search,” and selecting “UDM Search.” Through a UDM Search, Chronicle searches security data that has been ingested, parsed, and normalized. A UDM Search retrieves search results faster than a Raw Log Search because it searches through indexed and structured data that’s normalized in UDM.
![[chronicleudm.png]]

A UDM Search retrieves events formatted in UDM and these events contain UDM fields. There are many different types of UDM fields that can be used to query for specific information from an event. Discussing all of these UDM fields is beyond the scope of this reading, but you can learn more about UDM fields by exploring [Chronicle's UDM field list](https://cloud.google.com/chronicle/docs/reference/udm-field-list)

. Know that all UDM events contain a set of common fields including:

- **Entities**: Entities are also known as nouns. All UDM events must contain at least one entity. This field provides additional context about a device, user, or process that’s involved in an event. For example, a UDM event that contains entity information includes the details of the origin of an event such as the hostname, the username, and IP address of the event.
    
- **Event metadata**: This field provides a basic description of an event, including what type of event it is, timestamps, and more. 
    
- **Network metadata**: This field provides information about network-related events and protocol details. 
    
- **Security results**: This field provides the security-related outcome of events. An example of a security result can be an antivirus software detecting and quarantining a malicious file by reporting "virus detected and quarantined." 
    

Here’s an example of a simple UDM search that uses the event metadata field to locate events relating to user logins:

metadata.event_type = “USER_LOGIN” 

- metadata.event_type = “USER_LOGIN”: This UDM field metadata.event_type contains information about the event type. This includes information like timestamp, network connection, user authentication, and more. Here, the event type specifies USER_LOGIN, which searches for events relating to authentication. 
    

Using just the metadata fields, you can quickly start searching for events. As you continue practicing searching in Chronicle using UDM Search, you will encounter more fields. Try using these fields to form specific searches to locate different events.

### **Raw Log Search** 

If you can't find the information you are searching for through the normalized data, using a Raw Log Search will search through the raw, unparsed logs. You can perform a Raw Log Search by typing your search, clicking on “Search,” and selecting “Raw Log Search.” Because it is searching through raw logs, it takes longer than a structured search. In the Search field, you can perform a Raw Log Search by specifying information like usernames, filenames, hashes, and more. Chronicle will retrieve events that are associated with the search.

**Pro tip**: Raw Log Search supports the use of regular expressions, which can help you narrow down a search to match on specific patterns.

#### Example:
1. **VT CONTEXT**: This section provides the VirusTotal information available for the domain. 
    
2. **WHOIS**: This section provides a summary of information about the domain using WHOIS, a free and publicly available directory that includes information about registered domain names, such as the name and contact information of the domain owner. In cybersecurity, this information is helpful in assessing a domain's reputation and determining the origin of malicious websites. 
    
3. **Prevalence**: This section provides a graph which outlines the historical prevalence of the domain. This can be helpful when you need to determine whether the domain has been accessed previously. Usually, less prevalent domains may indicate a greater threat. 
    
4. **RESOLVED IPS**: This insight card provides additional context about the domain, such as the IP address that maps to signin.office365x24.com, which is 40.100.174.34. Clicking on this IP will run a new search for the IP address in Chronicle. Insight cards can be helpful in expanding the domain investigation and further investigating an indicator to determine whether there is a broader compromise.
    
5. **SIBLING DOMAINS**: This insight card provides additional context about the domain. Sibling domains share a common top or parent domain. For example, here the sibling domain is listed as login.office365x24.com, which shares the same top domain office365x24.com with the domain you're investigating: signin.office365x24.com.
    
6. **ET INTELLIGENCE REP LIST**: This insight card includes additional context on the domain. It provides threat intelligence information, such as other known threats related to the domains using ProofPoint's Emerging Threats (ET) Intelligence Rep List.
    
7. Click **TIMELINE**. This tab provides information about the events and interactions made with this domain. Click **EXPAND ALL** to reveal the details about the HTTP requests made including GET and POST requests.  A GET request retrieves data from a server while a POST request submits data to a server.
    
8. Click **ASSETS**. This tab provides a list of the assets that have accessed the domain.
![[chronicles.png]]

Information about the events and assets relating to the domain are separated into the two tabs: **TIMELINE** and **ASSETS**. **TIMELINE** shows the timeline of events that includes when each asset accessed the domain. **ASSETS** list hostnames, IP addresses, MAC addresses, or devices that have accessed the domain.

Investigate the affected assets and events by exploring the tabs:

1. **ASSETS**: There are several different assets that have accessed the domain, along with the date and time of access. Using your incident handler's journal, record the name and number of assets that have accessed the domain. 
    
2. **TIMELINE**: Click **EXPAND ALL** to reveal the details about the HTTP requests made, including GET and POST requests. The POST information is especially useful because it means that data was sent to the domain. It also suggests a possible successful phish. Using your incident handler's journal, take note of the POST requests to the /login.php page. For more details about the connections, open the raw log viewer by clicking the open icon.
![[asset&timeline.png]]
## Resources
- [Guide on getting data into Splunk](https://docs.splunk.com/Documentation/SplunkCloud/9.0.2303/Data/Howdoyouwanttoadddata)
- [Splunk’s Search Manual](https://docs.splunk.com/Documentation/Splunk/9.0.1/Search/GetstartedwithSearch)
- [Chronicle's quickstart guide](https://cloud.google.com/chronicle/docs/review-security-alert
- [Guide on data ingestion into Chronicle](https://cloud.google.com/chronicle/docs/data-ingestion-flow)